# from_scratch_Transformer_Neural_Network_Adaptable_to_GPT_or_BERT_English_to_Croatian_Translator
Using Pytorch I create from scratch a full Transformer model (with separate encoder, and decoder blocks that can then be stacked). This architecture can be easily then implemented to act as GPT or BERT. <br />
I here trained a full Transformer network to act as a translator from English to Croatian. Inside the EnglishCroatianDatabse is a database I created that has an English-to-Croatian pair of translated sentences, I created 1049773 such pairs and used them for my training but if needed anybody can use this database for their needs. <br />
<br />
Here I will shortly explain the process of the Transformer and the scripts inside the source directory:<br />


